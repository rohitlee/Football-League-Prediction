{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ANUVqGaukie"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from random import uniform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\""
      ],
      "metadata": {
        "id": "qEUjeYrjulF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "years = list(range(2025, 2020, -1))\n",
        "all_matches = []"
      ],
      "metadata": {
        "id": "_eUceAEsup4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each season\n",
        "for year in years:\n",
        "    data = requests.get(standings_url)\n",
        "    soup = BeautifulSoup(data.text, 'html.parser')\n",
        "\n",
        "    # Extract links to each team's page\n",
        "    standings_table = soup.select('table.stats_table')[0]\n",
        "    links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
        "    links = [l for l in links if '/squads/' in l]\n",
        "    team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
        "\n",
        "    # Find link to the previous season\n",
        "    previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
        "    standings_url = f\"https://fbref.com{previous_season}\"\n",
        "\n",
        "    # Loop through each team\n",
        "    for team_url in team_urls:\n",
        "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
        "        data = requests.get(team_url)\n",
        "        matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
        "        soup = BeautifulSoup(data.text, 'html.parser')\n",
        "\n",
        "        # Find relevant links for Shooting, Goalkeeping, Passing, etc.\n",
        "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
        "        links_dict = {\n",
        "            \"Shooting\": [l for l in links if l and 'all_comps/shooting/' in l],\n",
        "            \"Goalkeeping\": [l for l in links if l and 'all_comps/keeper/' in l],\n",
        "            \"Passing\": [l for l in links if l and 'all_comps/passing/' in l],\n",
        "            \"Goal and Shot Creation\": [l for l in links if l and 'all_comps/gca/' in l],\n",
        "            \"Defensive Actions\": [l for l in links if l and 'all_comps/defense/' in l]\n",
        "        }\n",
        "\n",
        "        # Create a function to load and merge data\n",
        "        def load_stats_data(url, match):\n",
        "            if url:\n",
        "                data = requests.get(f\"https://fbref.com{url[0]}\")\n",
        "                stat_table = pd.read_html(data.text)[0]\n",
        "                stat_table.columns = stat_table.columns.droplevel()  # Drop multi-level column headers\n",
        "                try:\n",
        "                    return match.merge(stat_table, on=\"Date\")\n",
        "                except ValueError:\n",
        "                    return match\n",
        "            return match\n",
        "\n",
        "        # Merge the Shooting table\n",
        "        matches = load_stats_data(links_dict[\"Shooting\"], matches)\n",
        "\n",
        "        # Merge the Goalkeeping table\n",
        "        matches = load_stats_data(links_dict[\"Goalkeeping\"], matches)\n",
        "\n",
        "        # Merge the Passing table\n",
        "        matches = load_stats_data(links_dict[\"Passing\"], matches)\n",
        "\n",
        "        # Merge the Goal and Shot Creation table\n",
        "        matches = load_stats_data(links_dict[\"Goal and Shot Creation\"], matches)\n",
        "\n",
        "        # Merge the Defensive Actions table\n",
        "        matches = load_stats_data(links_dict[\"Defensive Actions\"], matches)\n",
        "\n",
        "        # Filter for Premier League matches only\n",
        "        team_data = matches[matches[\"Comp\"] == \"Premier League\"]\n",
        "\n",
        "        # Add additional columns for season and team\n",
        "        team_data[\"Season\"] = year\n",
        "        team_data[\"Team\"] = team_name\n",
        "\n",
        "        # Append the team's data to the list\n",
        "        all_matches.append(team_data)\n",
        "\n",
        "        # Pause to avoid overwhelming the server\n",
        "        time.sleep(uniform(1, 180))  # Sleep for a random time between 1 and 55 seconds\n",
        "\n",
        "# Combine all matches into a single DataFrame\n",
        "final_df = pd.concat(all_matches, ignore_index=True)"
      ],
      "metadata": {
        "id": "ptAbtqoiLJcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_csv('dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "UvWB-1K8u66u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QfendPc1JR6M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}